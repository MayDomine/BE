{
    "class": "GPTDataset",
    "dataset_path": "data/codeparrot_content_document",
    "num_samples": 320,
    "index_split": "train",
    "random_seed": 1234,
    "sequence_length": 16384,
    "split": "98,2,0",
    "split_matrix": [
        [
            0,
            0.98
        ],
        [
            0.98,
            1.0
        ],
        null
    ],
    "tokenizer": {
        "class": "_GPT2BPETokenizer",
        "tokenizer_path": [
            "data/vocab.json",
            "data/merges.txt"
        ]
    }
}